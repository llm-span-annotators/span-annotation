annotation_span_categories:
- name: Contradictory
  description: The fact contradicts the data.
  color: rgb(214, 39, 40)
- name: Not checkable
  description: The fact cannot be verified from the data.
  color: rgb(148, 103, 189)
- name: Misleading
  description: The fact is technically true, but leaves out important information
    or otherwise distorts the context.
  color: rgb(230, 171, 2)
- name: Incoherent
  description: The text uses unnatural phrasing or does not fit the discourse.
  color: rgb(140, 86, 75)
- name: Repetitive
  description: The fact has been already mentioned earlier in the text.
  color: rgb(27, 158, 119)
- name: Other
  description: The text is problematic for another reason.
  color: rgb(102, 102, 102)
prompt_template: "Your task is to identify errors in the text and classify them.\n\
  \nOutput the errors as a JSON object with a single key \"annotations\". The value\
  \ of \"annotations\" is a list in which each object contains fields \"reason\",\
  \ \"text\", and \"annotation_type\". The value of \"reason\" is the short sentence\
  \ justifying the annotation. The value of \"text\" is the literal value of the identified\
  \ span (we will later identify the span using string matching). The value of \"\
  annotation_type\" is an integer index of the error based on the following list:\n\
  \n0: Contradictory (The fact contradicts the data.)\n1: Not checkable (The fact\
  \ cannot be verified from the data.)\n2: Misleading (The fact is technically true,\
  \ but leaves out important information or otherwise distorts the context.)\n3: Incoherent\
  \ (The text uses unnatural phrasing or does not fit the discourse.)\n4: Repetitive\
  \ (The fact has been already mentioned earlier in the text.)\n5: Other (The text\
  \ is problematic for another reason.)\n\nHints:\n- Always try to annotate the longest\
  \ continuous span (i.e., the whole fact instead of a single word).\n- Annotate only\
  \ the spans that you are sure about. If you are not sure about an annotation, skip\
  \ it.\n- Ignore subjective statements: for example \"a lightweight smartphone\"\
  \ highly depends on the context: you should not annotate these statements.\n- Numerical\
  \ conventions: For weather forecasts, we accept both precise numbers (e.g. 10.71\xB0\
  C) and the rounded ones (e.g. 11\xB0C) as long as they agree with the data.\n- Annotate\
  \ only according to your own knowledge. If you are considering using an external\
  \ tool (Google, ChatGPT etc.), just skip that specific fact.\n\nIf there is nothing\
  \ to annotate in the text, \"annotations\" will be an empty list.\n\nGiven the data:\n\
  ```\n{data}\n```\nannotate the errors in the corresponding text generated from the\
  \ data:\n```\n{text}\n```"
model: deepseek-r1:70b
model_args: {}
campaign_orig_id: gold-final-noexamples-deepseek-r1
